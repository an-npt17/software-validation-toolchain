\chapter{Evaluation}

\section{Benchmark Suite}

We evaluated the toolchain on standard verification benchmarks:

\begin{itemize}
	\item \textbf{array-cav19}: 13 array manipulation benchmarks from CAV 2019
	\item \textbf{float-newlib}: 265 floating-point benchmarks from Newlib
	\item \textbf{misc-sv-benchmarks}: Additional SV-COMP benchmarks
\end{itemize}

\section{Experimental Setup}

\textbf{TO COMPLETE - You should provide:}
\begin{itemize}
	\item Hardware specifications (CPU, RAM)
	\item Operating system version
	\item Timeout settings used
	\item Number of parallel jobs
	\item Total runtime for full benchmark suite
\end{itemize}

\section{Results}

Table \ref{tab:results} shows preliminary results from the benchmark run:

\begin{table}[htbp]
	\caption{Benchmark Results Summary}
	\label{tab:results}
	\centering
	\begin{tabular}{@{}lrrr@{}}
		\toprule
		\textbf{Metric}      & \textbf{Value} \\
		\midrule
		Total Benchmarks     & 23             \\
		Total Time           & 1212.99s       \\
		Average Time         & 52.74s         \\
		\midrule
		Safe Verdicts        & 0 (0.0\%)      \\
		Unsafe Verdicts      & 19 (82.6\%)    \\
		Unknown Verdicts     & 4 (17.4\%)     \\
		\midrule
		Frama-C Success      & 10/23 (43.5\%) \\
		CBMC Success         & 23/23 (100\%)  \\
		KLEE Success         & 0/23 (0\%)     \\
		\midrule
		Accuracy vs Expected & 4/23 (17.4\%)  \\
		\bottomrule
	\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{itemize}
	\item CBMC successfully completed all benchmarks
	\item Frama-C timed out on 13 array benchmarks (primarily loop-heavy programs requiring invariants)
	\item KLEE encountered compilation issues (bitcode generation)
	\item Most benchmarks correctly identified as unsafe
\end{itemize}

\section{AI Translation Accuracy}

\textbf{TO COMPLETE - You should evaluate:}
\begin{itemize}
	\item Test the nl-to-acsl tool on a set of requirements
	\item Manually review generated ACSL for correctness
	\item Calculate accuracy metrics (e.g., 20 test cases, X correct)
	\item Measure confidence score correlation with actual correctness
	\item Example test cases:
	      \begin{itemize}
		      \item "Buffer must not overflow" $\rightarrow$ \texttt{requires \textbackslash valid(buf+(0..n-1))}
		      \item "Pointer must not be null" $\rightarrow$ \texttt{requires ptr != \textbackslash null}
		      \item "Return value is positive" $\rightarrow$ \texttt{ensures \textbackslash result > 0}
	      \end{itemize}
\end{itemize}

\section{Case Study: Banking System}

\textbf{TO COMPLETE - You should document:}
\begin{itemize}
	\item Description of the banking system code (src/banking\_system.c)
	\item Requirements extracted/written
	\item ACSL specifications generated
	\item Verification results from each tool
	\item Bugs found (if any)
	\item Proof completion rates
\end{itemize}

\section{Discussion}

\subsection{Strengths}

\begin{itemize}
	\item \textbf{Comprehensive Coverage}: Multiple verification approaches (static + dynamic) increase bug detection
	\item \textbf{Automation}: AI-powered translation reduces manual effort
	\item \textbf{Reproducibility}: Nix ensures consistent environments
	\item \textbf{Flexibility}: Modular design allows using tools independently
\end{itemize}

\subsection{Limitations}

\begin{itemize}
	\item \textbf{Loop Invariants}: Frama-C requires manual loop invariants for complex array operations
	\item \textbf{KLEE Bitcode}: Some programs fail to compile to LLVM bitcode
	\item \textbf{AI Hallucinations}: LLM may generate syntactically correct but semantically wrong specifications
	\item \textbf{Timeout Issues}: Complex programs may exceed time limits
\end{itemize}

\subsection{Comparison with Related Work}

\textbf{TO COMPLETE - You should compare:}
\begin{itemize}
	\item Compare with other integrated verification tools (SeaHorn, Ultimate Automizer)
	\item Compare AI translation accuracy with other NL-to-formal methods
	\item Discuss advantages of multi-tool approach vs single-tool
	\item Benchmark performance comparison (if data available)
\end{itemize}

\section{Lessons Learned and Best Practices}

\subsection{Effective AI Prompting}

Through experimentation, we found several prompt engineering strategies effective:

\begin{itemize}
	\item \textbf{Structured Output}: Requesting JSON format improves parsing reliability
	\item \textbf{Examples}: Providing 3-5 examples significantly improves accuracy
	\item \textbf{Context}: Including function signatures and variable types helps
	\item \textbf{Confidence Scores}: Allows filtering low-confidence results for manual review
\end{itemize}

\subsection{Tool Combination Strategy}

We recommend the following verification strategy:

\begin{enumerate}
	\item \textbf{Quick checks}: CBMC for fast bounded checking
	\item \textbf{Deep verification}: Frama-C for critical functions
	\item \textbf{Bug hunting}: KLEE + AFL++ for finding edge cases
	\item \textbf{Memory safety}: Valgrind for runtime analysis
\end{enumerate}

\subsection{Specification Writing Guidelines}

For best results with AI translation:

\begin{itemize}
	\item Write requirements as concrete statements (not vague goals)
	\item Include variable names and types when possible
	\item Separate preconditions from postconditions
	\item Use consistent terminology
	\item Example good requirement: "The buffer of size n must have valid indices from 0 to n-1"
\end{itemize}
